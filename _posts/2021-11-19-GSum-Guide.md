---
title: "GSum ì‹¤í–‰ ê°€ì´ë“œ"
date: 2021-11-19 10:14:37 -0400
categories: NLP
tag : Text-Summarization
use_math: true

---

<br>

# **GSum ì‹¤í–‰ ê°€ì´ë“œ**

### **ğŸ“„Paper: GSum (Dou et al., 2021, NAACL)** 
GSum: A General Framework for Guided Neural Abstractive Summarization

### **ğŸ’» Github Repo**  
https://github.com/neulab/guided_summarization


<br>

| Dataset | Source | Type | Train size | Valid size | Test size | Document Token | Summary Token |
|--|--|--|--|--|--|--|--|
| Reddit (TIFU-long) | Social Media | SDS | 41675 | 645 | 645 | (avg.) 482.2 |  (avg.) 28.0 |
| XSum | News | SDS | 203028 | 11273 | 11257 | (avg.) 430.2 |  (avg.) 23.3 |
| CNN/DailyMail (Non_Anonymized) | News | SDS | 287084 | 13367 | 11489 | (avg.) 766.1 |  (avg.) 58.2 |
| WikiHow | Knowledge Base | SDS | 168126 | 6000 | 6000 | (avg.) 580.8 |  (avg.) 62.6 |
| New York Times | News | SDS |  |  |  |  |   |
| PubMed | Scientific Paper | SDS | 83233 | 4946 | 5025 | (avg.) 444.0 |  (avg.) 209.5 |


> ğŸ“¢ **SDS** : Single Document Summary  

NYT ë°ì´í„°ì…‹ì€ Licenseê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì •í™•í•œ ì •ë³´ë¥¼ íŒŒì•…í•˜ì§€ ëª»í–ˆë‹¤.  

<br>

```python
## Dependency

multiprocess==0.70.9
numpy==1.17.2
pyrouge==0.1.3
pytorch-transformers==1.2.0
tensorboardX==1.9
torch==1.1.0
fairseq==0.10.2
rouge==1.5.5
```

ğŸ“¢ Dou et al.ì˜ ì—°êµ¬ì—ì„œëŠ” **BertAbs** (Liu and Lapata, 2019)ì™€ **BART** (Lewis et al., 2020) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ì„±ëŠ¥ì„ ì¸¡ì •í•¨

<br>

---

## **BertAbs ê¸°ë°˜ Guidance Summarization**
<br>

ğŸ“£ í˜„ì¬ ê²½ë¡œ `<home>/guided_summarization/bert`

<br>

- ### **Download Preprocessed Dataset**
    
    Liu and Lapataì˜ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, [***PreSumm repository***](https://github.com/nlpyang/PreSumm)ì—ì„œ ë°°í¬í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•¨
    
    [bert_data_cnndm_final.zip](https://drive.google.com/file/d/1DN7ClZCCXsk2KegmC6t4ClBwtAf5galI/view?usp=drivesdk)
    
- ### **ëª¨ë¸ í•™ìŠµ**
    
    ê²½ë¡œ ì„¤ì •  
    ```bash
    export DATA_DIR_PATH=/<processed data directory path>/
    export MODEL_DIR_PATH=/<model directory path>/
    export LOG_PATH=/<training log path>/training.log
    export DATA_PATH=$DATA_DIR_PATH/cnndm
    ```

    <br>

    > `$DATA_DIR_PATH` ë‚´ ë°ì´í„°ë“¤ì€ `cnndm.train.1.bert.pt`ì˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
    > 
    
    <br>

    ğŸ“¢ BERT ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´, ì²«ë²ˆì§¸ ì‹¤í–‰ì¼ ê²½ìš° ì•„ë˜ ì»¤ë§¨ë“œì˜ `visible_gpus` ë¥¼ -1ë¡œ ì…ë ¥í•œë‹¤.  
    ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ëë‚˜ë©´ ì¢…ë£Œí•œ í›„, í™˜ê²½ì— ë§ê²Œ multi-GPUë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.
    
   
    
    ```bash
    python z_train.py -task abs -mode train \
    				-bert_data_path $DATA_PATH \
    				-dec_dropout 0.2 \
    				-model_path $MODEL_PATH \
    				-sep_optim true \
    				-lr_bert 0.002 \
    				-lr_dec 0.2 \
    				-save_checkpoint_steps 2000 \
    				-batch_size 140 \
    				-train_steps 200000 \
    				-report_every 50 \
    				-accum_count 5 \
    				-use_bert_emb true \
    				-use_interval true \
    				-warmup_steps_bert 20000 \
    				-warmup_steps_dec 10000 \
    				-max_pos 512 \
    				-visible_gpus 0,1 \ # -1 for first run 
    				-log_file $LOG_PATH
    ```
    
    <br>

- ### **ëª¨ë¸ í…ŒìŠ¤íŠ¸**
    
    
    ìƒì„±ëœ ìš”ì•½ë¬¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
    
    ```bash
    export RESULT_PATH=/<generated summaries path>/
    ```
    
    
    ğŸ“¢ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œ, GPUëŠ” í•˜ë‚˜ë§Œ ì‚¬ìš©
    
    
    ```bash
    python -W ignore z_train.py \
        -task abs \
        -mode test \
        -batch_size 3000 \
        -test_batch_size 1500 \
        -bert_data_path $DATA_PATH \
        -log_file logs/test.logs \
        -test_from $MODEL_PATH \
        -sep_optim true \
        -use_interval true \
        -visible_gpus 0 \
        -max_pos 512 \
        -max_length 200 \
        -alpha 0.95 \
        -min_length 50 \
        -result_path $RESULT_PATH
    ```
    
<br>

---

## **BART ê¸°ë°˜ Guidance Summarization**
<br>

ğŸ“£ í˜„ì¬ ê²½ë¡œ `<home>`

<br>

- ### **ë°ì´í„° ì „ì²˜ë¦¬**
    <br>
    
    **Download Source Dataset and Guidance**
    
    BART ëª¨ë¸ì„ ìœ„í•œ Non-Anonymized CNN/DailyMail ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´
     [***sensim repository***](https://github.com/icml-2020-nlp/semsim)ì—ì„œ ë°°í¬í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•¨
    
    [cnn/dailymail dataset zipfile](https://drive.google.com/file/d/1goxgX-0_2Jo7cNAFrsb9BJTsrH7re6by/view?usp=drivesdk)  
    [guidance zipfile for cnn/dailymail](https://drive.google.com/file/d/12SpWwfD3syIxcC-SdSNnDOI5sbXJaylC/view?usp=drivesdk)
    
    <br>

    **Encode with GPT2-BPE**
    
    
    
    ```bash
    git clone https://github.com/pytorch/fairseq.git  
    cd fairseq # í˜„ì¬ ê²½ë¡œ <home>/fairseq

    wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json  
    wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe
    wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt
    ```

    ```bash
    export DATA_DIR_PATH=/<path to downloaded data>/cnn_dm
    export GUIDANCE_DIR_PATH=/<path to downloaded guidance>
    ```

    `$DATA_DIR_PATH` ê²½ë¡œ ë‚´ `train.source`, `train.target`, `val.source`, `val.target` ë°ì´í„°ì— Byte-pair Encoding
    
    ```bash
    for SPLIT in train val
    	do
    		for LANG in source target
    		do
    			python -m examples.roberta.multiprocessing_bpe_encoder \
    			--encoder-json encoder.json \
    			--vocab-bpe vocab.bpe \
    			--inputs $DATA_DIR_PATH/$SPLIT.$LANG \
    			--outputs $DATA_DIR_PATH/$SPLIT.bpe.$LANG \
    			--workers 60 \
    			--keep-empty
    	done
    done
    ```
    
    `$GUIDANCE_DIR_PATH` ê²½ë¡œ ë‚´ `train.oracle`, `val.oracle` ë°ì´í„°ì— Byte-pair Encoding
    
    ```bash
    for SPLIT in train val
    	do 
    	python -m examples.roberta.multiprocessing_bpe_encoder \
    	--encoder-json encoder.json \
    	--vocab-bpe vocab.bpe \
    	--inputs $GUIDANCE_DIR_PATH/$SPLIT.oracle \
    	--outputs $DATA_DIR_PATH/$SPLIT.bpe.z \
    	--workers 60 \
    	--keep-empty
    done
    ```
    
    **Binarize Dataset**
    
    `guided_summarization` ë””ë ‰í† ë¦¬ë¡œ ì´ë™ (í˜„ì¬ ê²½ë¡œ `<home>/guided_summarization` )
    
    ```bash
    wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt
    ```

    ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
    
    ```bash
    export BIN_DIR_PATH=/<output binarized data directory path>/
    ```

    ```bash
    python fairseq_cli/guided_preprocess.py \
      --source-lang source \
      --target-lang target \
      --trainpref **$DATA_DIR_PATH**/train.bpe \
      --validpref **$DATA_DIR_PATH**/val.bpe \
      --destdir **$BIN_DIR_PATH** \
      --workers 60 \
      --srcdict dict.txt \
      --tgtdict dict.txt
    ```
    
    <br>

- **ëª¨ë¸ í•™ìŠµ**
    
    
    í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •

    ```bash
    export MODEL_PATH=/<model save path>/
    export LOG_PATH=/<training log directory path>/training.log
    export BART_PATH=<checkpoint filename>
    ```

    (í•™ìŠµëœ ëª¨ë¸ì€ `$MODEL_PATH/$BART_PATH`ì˜ ê²½ë¡œì— ì €ì¥ë¨)
    
    ```bash
    export TOTAL_NUM_UPDATES=20000
    export WARMUP_UPDATES=500
    export LR=3e-05
    export MAX_TOKENS=2048
    export UPDATE_FREQ=16
    ```

    ```bash
    CUDA_VISIBLE_DEVICES=0,1 python train.py $BIN_DIR_PATH\
        --restore-file $BART_PATH \
        --max-tokens $MAX_TOKENS \
        --task guided_translation \
        --source-lang source --target-lang target \
        --truncate-source \
        --layernorm-embedding \
        --share-all-embeddings \
        --share-decoder-input-output-embed \
        --reset-optimizer --reset-dataloader --reset-meters \
        --required-batch-size-multiple 1 \
        --arch guided_bart_large \
        --criterion label_smoothed_cross_entropy \
        --label-smoothing 0.1 \
        --dropout 0.1 --attention-dropout 0.1 \
        --weight-decay 0.01 --optimizer adam --adam-betas "(0.9, 0.999)" --adam-eps 1e-08 \
        --clip-norm 0.1 \
        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \
        --fp16 --update-freq $UPDATE_FREQ \
        --skip-invalid-size-inputs-valid-test \
        --save-dir $MODEL_PATH \
        --find-unused-parameters
    ```
    
    - í•™ìŠµëœ ëª¨ë¸    
        [bart_sentence.pt](https://drive.google.com/file/d/1BMKhAh2tG5p8THxugZWMPc7NXqwJDHLw/view?usp=drivesdk)
        
    <br>

- **ëª¨ë¸ í…ŒìŠ¤íŠ¸**
    
    
    **í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì„¤ì •**
    
    ```bash
    export TEST_DATA_PATH=/<test source data path>/
    export TEST_GUIDANCE=/<test guidance path>/
    export RESULT_PATH=/<generated summaries path>/
    export MODEL_DIR=/<trained model path>/
    ```

    *Optional Parameter*
    
    ```bash
    export MODEL_NAME=/<trained model name>/ # default : model.pt  
    export DATA_BIN=/<trained data path>/ # default : .
    ```

    <br>

    **ìš”ì•½ë¬¸ ìƒì„±**
    
    ```bash
    python z_test.py $TEST_DATA_PATH $TEST_GUIDANCE $RESULT_PATH $MODEL_DIR $MODEL_NAME $DATA_BIN
    ```
    <br>

    **ê²°ê³¼ íŒŒì¼ë¡œë¶€í„° ROUGE score ê³„ì‚°**
    
    1) **Files2ROUGE ë‹¤ìš´ë¡œë“œ**

    ```bash
    # Install Requirement
    pip install -U git+https://github.com/pltrdy/pyrouge
    ```
        

    ```bash
    # Clone the repo, setup the module and ROUGE
    git clone https://github.com/pltrdy/files2rouge.git
    cd files2rouge
        
    python setup_rouge.py
    python setup.py install
    ```
    
    2) **Stanford CoreNLP 4.2.2 (for tokenize) ë‹¤ìš´ë¡œë“œ**  
    
    [Stanford CoreNLP 4.2.2 ë‹¤ìš´ë¡œë“œ](https://stanfordnlp.github.io/CoreNLP/)
    
    
    ```bash
    export CLASSPATH=/<corenlp_download_path>/stanford-corenlp-4.2.2.jar
    ```
    
    3) **ROUGE score ê³„ì‚°**   
    ğŸ“¢ ROUGE-1.5.5 ë²„ì „ì„ ì„¤ì¹˜í•´ì•¼í•¨  
        
    ```bash
    cat test.hypo | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.tokenized
    cat test.target | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.target

    files2rouge test.hypo.tokenized test.hypo.target
    ```
