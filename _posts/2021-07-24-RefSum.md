---
title: "RefSum ë…¼ë¬¸ ë¦¬ë·°"
date: 2021-07-24 21:22:36 -0400
categories: NLP
tag : Text-Summarization
---


# **RefSum (Liu et al., 2021, NAACL)**

[ğŸ“„**Paper : RefSum: Refactoring Neural Summarization**](https://aclanthology.org/2021.naacl-main.113/)

> ê¸°ì¡´ 2-Stage Learningì˜ í•œê³„ë¥¼ ì™„í™”í•œ íŒ¨ëŸ¬ë‹¤ì„ ì œì•ˆ

**stage ê°„ì˜ parameter ê³µìœ , _pretrain-then-finetune_** â†’ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŒ

<br>

2-Stage learningì—ëŠ” ì•„ë˜ 2ê°€ì§€ í˜•íƒœë¡œ ë‚˜ë‰œë‹¤.
- ***stacking*** : ì„œë¡œ ë‹¤ë¥¸ ì—¬ëŸ¬ base ëª¨ë¸ë“¤ë¡œë¶€í„° ìƒì„±ëœ candidate summaryë“¤ì„ í†µí•´ meta ëª¨ë¸ì´ ìµœì¢… summaryë¥¼ ìƒì„±  
- ***re-ranking*** : í•˜ë‚˜ì˜ base ëª¨ë¸ë¡œë¶€í„° ìƒì„±ëœ ì—¬ëŸ¬ candidate summaryë“¤ì„ í†µí•´ meta ëª¨ë¸ì´ ìµœì¢… summary ìƒì„±

<br>

## ğŸ’¡ **Contribution ì •ë¦¬**

### 1.  ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•œ 2-stage learningì˜ í•œê³„ ë¶„ì„

<br>
ê¸°ì¡´ 2-stage learningì€ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ë¥¼ ê°€ì§€ë©°, ì´ë¡œ ì¸í•´ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì˜¨ì „íˆ í™œìš©í•˜ê¸° ì–´ë ¤ì›€  

-   **Base-Meta Learning Gap** Base modelê³¼ Meta model ê°„ parameter sharingì˜ ë¶€ì¬ë¡œ ì¸í•´ ë°œìƒ â†’ Meta modelì€ Base modelì˜ outputì„ ì˜¨ì „íˆ í™œìš©í•˜ì§€ ëª»í•¨
-   **Train-Test Distribution Gap** í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ Meta modelì˜ output distributionê³¼ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ output distributionì€ ì°¨ì´ê°€ ìˆìŒ â†’ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ output distributionì´ ë” ì •í™•í•¨
<br>
<br>

<div align=center>
<img src="/assets/images/refsum/intro-gap.PNG" width=550/><br>
</div>
<br>

### 2.  ë‘ Gapì„ ì™„í™”í•œ íŒ¨ëŸ¬ë‹¤ì„ ì œì•ˆ

<br>

***Refactor***

-   Base modelì´ë©´ì„œ, Meta modelë¡œë„ í™œìš©í•  ìˆ˜ ìˆì–´ *Parameter sharing*ì´ ê°€ëŠ¥í•¨ â†’ *Base-Meta Gap* ì™„í™”
-   **2ë²ˆì˜ í•™ìŠµ**ìœ¼ë¡œ ë‹¤ì–‘í•œ candidate summaryë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŒ â†’ *Train-Test Gap* ì™„í™”

    â‘  _pre-train_ : Input documentë¡œë¶€í„° candidate summary ìƒì„±  
    â‘¡ _fine-tune_ : Base modelì˜ ë‹¤ì–‘í•œ outputìœ¼ë¡œë¶€í„° new candidate summary ìƒì„±

<br>

RefactorëŠ” Base modelê³¼ Meta modelì„ ë¶„ë¦¬í•˜ì§€ ì•Šê³  ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ parameter sharingì´ ê°€ëŠ¥í•¨

*ê¸°ì¡´ì˜ 2 stage learningì˜ ë¶„ë¦¬ëœ base-meta ëª¨ë¸*
$C = BASE(D, \mathcal{T}, S, \theta^{base}) \newline  
C^{*} = META(D, \mathcal{C}, \theta^{meta})$

*Refactorë¥¼ í†µí•´ í†µí•©ëœ base-meta model*  
$C_{*} = REFACTOR(D,\mathcal{C}, \theta^{refactor}) \newline
$

<br>

ì•„ë˜ëŠ” Liu et al.ì˜ ì‹¤í—˜ì„ ìš”ì•½í•œ ë‚´ìš©ì´ë‹¤.  
> base ëª¨ë¸ë§Œ ì‚¬ìš©í•œ ê²½ìš° (*Base*)  
> pre-trained Refactorë¥¼ meta ëª¨ë¸ë¡œ ì¶”ê°€í•œ ê²½ìš° (*Pre-trained*)  
> Refactorë¥¼ meta ëª¨ë¸ë¡œ ì¶”ê°€í•˜ì—¬ base modelì˜ outputìœ¼ë¡œë§Œ í•™ìŠµí•œ ê²½ìš° (*Supervised*)  
> pre-trained Refactorë¥¼ meta ëª¨ë¸ë¡œ ì¶”ê°€í•œ í›„, fine-tuning (*Fine-tuned*) 

<br>

1) Single System re-ranking (on CNN/DM)

    base ëª¨ë¸ : BART/GSum  
    meta ëª¨ë¸ : pre-trained Refactor / Supervied Refactor / fine-tuned Refactor  

<br>
<div align=center>
<img src="/assets/images/refsum/single-reranking.PNG" width=340/><br>
</div>
<br>

> Refactorë¥¼ meta ëª¨ë¸ë¡œ ì‚¬ìš©í–ˆì„ ë•Œ, base ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë›°ì–´ ë„˜ì„ ìˆ˜ ìˆë‹¤.    
> fine-tuningì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.

<br>

2) Multi System Stacking (on CNN/DM)  

    - **Summary-level combination** of base model outputs  
            base ëª¨ë¸ : BART & pre-trained Refactor (*Two*)/ BART & GSum & pre-trained Refactor (*Three*)  
            meta ëª¨ë¸ : pre-trained Refactor / Supervied Refactor / fine-tuned Refactor   

        ê° base modelì˜ outputì„ summary-levelë¡œ combination  

    - **Sentence-level combination** of base model outputs  
            base ëª¨ë¸ : BART & pre-trained Refactor  
            meta ëª¨ë¸ : pre-trained Refactor / Supervied Refactor / fine-tuned Refactor  

        ê° base modelì˜ outputì„ sentence-levelë¡œ combination
<br>
<img src="/assets/images/refsum/multi-stacking.PNG" width=340/> <img src="/assets/images/refsum/multi-stacking-sent-level.PNG" width=340/>
<br>

<br>

3) Generalization on 19 Top-performing System (on CNN/DM)
    base ëª¨ë¸ : 19ê°œì˜ Top-performing system  
    meta ëª¨ë¸ : pre-trained Refactor

    ì´ ì‹¤í—˜ì—ì„œ meta ëª¨ë¸ì¸ pretrained RefactorëŠ” fine-tuning ì—†ì´ base ëª¨ë¸ì˜ outputì„ í†µí•´ summaryë¥¼ ì¶”ì¶œí•˜ì˜€ë‹¤.  
    19ê°œì˜ ëª¨ë¸ì„ ì¡°í•©í•˜ë©° multi-system stacking ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ê³ , ê²°ë¡ ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ë“¤ì´ base ëª¨ë¸ë¡œì¨ ì‚¬ìš©ë˜ì—ˆì„ ë•Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

    > ì•„ë˜ ê·¸ë¦¼ì—ì„œ x ì¶•ì€ base ëª¨ë¸ë¡œ ì‚¬ìš©ëœ systemë“¤ ì‚¬ì´ì˜ ì„±ëŠ¥ ì°¨ì´ì´ë©°, ROUGE-1 scoreë¥¼ í†µí•´ ì¸¡ì •ë˜ì—ˆë‹¤.  
    > yì¶•ì€ Refactorë¥¼ í†µí•œ single base modelì„ ì‚¬ìš©í•œ ê²½ìš° ëŒ€ë¹„ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒë¥ ì„ ì˜ë¯¸í•œë‹¤.

    <br>
<div align=center>
<img src="/assets/images/refsum/experiment-3.PNG" width=340/><br>
</div>
<br>

> base ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ ì ì„ ìˆ˜ë¡ Refactorë¥¼ ì¶”ê°€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.   

<br>

4) Effectiveness on X-Sum  
    base ëª¨ë¸ : BART, PEGASUS  
    meta ëª¨ë¸ : pre-trained Refactor / fine-tuned Refactor  

    *ì‹¤í—˜ 1 ê³¼ ë¹„ìŠ·í•œ ê²°ê³¼ ë„ì¶œ*

<br>

5) Fine-grained Analysis
    base ëª¨ë¸ : BART & pre-trained Refactor  
    meta ëª¨ë¸ : fine-tuned Refactor  

    ë‘ base ëª¨ë¸ì¸ BARTì™€ pre-trained Refactorê°€ ìƒì„±í•œ candidate summaryì˜ ROUGE score ì°¨ì´ (performance gap)ì— ë”°ë¥¸ meta ëª¨ë¸ì˜ ì •í™•ë„ ì¸¡ì •

    <br>
<div align=center>
<img src="/assets/images/refsum/experiment-5.PNG" width=340/><br>
</div>
<br>

performance gapì´ í´ìˆ˜ë¡ meta modelì˜ ì •í™•ë„ê°€ í–¥ìƒë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


performance gapì´ í¬ë‹¤ëŠ” ê²ƒì€ ì–´ëŠ í•œ ëª¨ë¸ì˜ ROUGE scoreê°€ ë‚®ê³  ë‹¤ë¥¸ ëª¨ë¸ì˜ ROUGE scoreê°€ ë†’ë‹¤ëŠ” ê²ƒì¸ë°, 


ì´ë¥¼ í†µí•´ ë‘ base ëª¨ë¸ì´ ì„œë¡œ ìƒí˜¸ë³´ì™„ì ìœ¼ë¡œ ë™ì‘í•˜ê³  ìˆìŒì„ ì§ì‘í•  ìˆ˜ ìˆë‹¤.

> ì–´ëŠ í•œ ëª¨ë¸ì˜ ROUGE scoreê°€ ê³„ì† ë‚®ê³ , ë‹¤ë¥¸ ëª¨ë¸ì´ ë†’ì€ scoreë¥¼ ê°€ì§€ì§€ ì•Šë‚˜ ìƒê°í•´ë³¼ ìˆ˜ ìˆê² ì§€ë§Œ, ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ê²ƒì€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ë“¤ì„ base ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—(ì‹¤í—˜ 3) ìƒí˜¸ë³´ì™„ì ìœ¼ë¡œ ë™ì‘í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤


<br>

ë…¼ë¬¸ì„ ì½ê³  ì´í•´ê°€ ê°€ì§€ ì•ŠëŠ” ë¶€ë¶„ì´ ìˆëŠ”ë°,

ë…¼ë¬¸ì—ì„œ Base-Meta ê°„ Gapì´ 2 stage learning ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ëŠ” ì£¼ìš” ìš”ì¸ì´ë¼ê³  ì–¸ê¸‰í•˜ì˜€ìœ¼ë©°,  
ì´ gapì„ ì™„í™”í•  REFACTORë¥¼ ë„ì…í•˜ì˜€ë‹¤.

í•˜ì§€ë§Œ, ì‹¤í—˜ ë‚´ìš©ì„ ë³´ë©´ Base-Meta ê°„ Gapì€ í•´ê²°ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤.   
Train-Test gapì€ Fine-tuningì„ í†µí•´ í•´ì†Œë  ìˆ˜ ìˆì§€ë§Œ, Base-Meta ê°„ gapì€ Parameter sharingì„ í†µí•´ í•´ì†Œë˜ì–´ì•¼ í•˜ëŠ”ë° (ë…¼ë¬¸ì—ì„œ ì£¼ì¥í•œ ë°”ì— ì˜í•˜ë©´), ì‹¤í—˜ ë‚´ìš©ì„ ë³´ë©´ RefactorëŠ” meta ëª¨ë¸ë¡œë§Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ëŒ€ë‹¤ìˆ˜ ì´ë©°, ì‹¤í—˜ 2ì—ì„œ Base ëª¨ë¸ë¡œë„ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ, ì´ë•Œ base ëª¨ë¸ì€ Refactor ì™¸ì—ë„ GSum, BARTë„ ì‚¬ìš©ë˜ì—ˆë‹¤.  
Refactorê°€ ì•„ë‹Œ ë‹¤ë¥¸ base ëª¨ë¸ê³¼ meta ëª¨ë¸ì€ Parameter Sharingì´ ì´ë¤„ì§€ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ë©°, BARTì™€ GSumì˜ outputì„ meta ëª¨ë¸ì´ ì˜¨ì „íˆ ì´ìš©í•œ ê²ƒì¸ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.




